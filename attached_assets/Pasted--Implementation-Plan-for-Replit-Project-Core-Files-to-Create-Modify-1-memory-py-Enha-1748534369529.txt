### Implementation Plan for Replit Project

#### Core Files to Create/Modify
1. `memory.py` (Enhanced Memory System)
2. `main.py` (Updated Application Logic)
3. `utils.py` (New Helper Functions)
4. `.env` (Additional Configuration)

### memory.py Update
```python
from neo4j import GraphDatabase
import os
import threading
import time
from datetime import datetime, timedelta
import pytz
from utils import generate_embedding, summarize_memory_cluster
import openai

# Configure OpenAI (OpenRouter) connection
openai.api_base = "https://openrouter.ai/api/v1"
openai.api_key = os.getenv("OPENROUTER_API_KEY")

class Neo4jMemory:
    def __init__(self):
        self.driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USER"), 
                 os.getenv("NEO4J_PASSWORD"))
        )
        self._init_db()
        self.background_worker = threading.Thread(target=self.run_background_tasks, daemon=True)
        self.background_worker.start()
        
    def _init_db(self):
        # Create indexes for performance
        with self.driver.session() as session:
            session.run("""
            CREATE INDEX causal_idx FOR ()-[r:CAUSALLY_RELATED]-() ON (r.confidence, r.detected_at)
            CREATE FULLTEXT INDEX chunk_content FOR (c:MemoryChunk) ON EACH [c.content]
            CREATE INDEX memory_type FOR (m:Memory) ON (m.type)
            CREATE INDEX principle_type FOR (c:MemoryChunk) ON (c.type)
            """)
    
    def run_background_tasks(self):
        """Run periodic optimization tasks in background"""
        while True:
            now = datetime.now(pytz.utc)
            
            if now.hour == 3 and now.minute < 5:  # 3:00-3:05 AM UTC
                for user_id in self.get_active_users():
                    self.create_memory_chunks(user_id)
                    self.detect_contradictions(user_id)
                    self.optimize_connections(user_id)
            
            # Run hourly tasks
            self.prune_low_confidence_memories()
            time.sleep(1800)  # Check every 30 minutes
    
    def get_active_users(self):
        """Get list of active users (simplified)"""
        with self.driver.session() as session:
            result = session.run("""
            MATCH (u:User)
            RETURN u.id AS user_id
            """)
            return [record['user_id'] for record in result]
    
    # Existing functions from previous implementation...
    
    def store_chat(self, user_id, role, content):
        # ... existing store_chat code ...
        
        # Add simple causal analysis when trigger words detected
        causal_triggers = ["because", "therefore", "so", "since", "as a result"]
        if any(trigger in content.lower() for trigger in causal_triggers):
            self.link_causal_memories(user_id, new_memory_id, content)

    # === NEW FEATURE 1: MEMORY COMPRESSION ===
    def create_memory_chunks(self, user_id):
        """Nightly memory consolidation"""
        with self.driver.session() as session:
            # Find memory clusters
            clusters = session.run("""
            MATCH (u:User {id: $user_id})-[:CREATED]->(m:Memory)
            WHERE m.timestamp < datetime() - duration('P1D')
            AND NOT EXISTS ( (m)-[:SUMMARIZED_IN]->(:MemoryChunk) )
            WITH m ORDER BY m.timestamp DESC
            CALL {
                WITH m
                CALL db.index.vector.queryNodes('memory_embeddings', 5, m.embedding)
                YIELD node AS similar
                WHERE similar <> m
                RETURN collect(similar) AS cluster
            }
            WITH m, cluster
            WHERE size(cluster) >= 3
            RETURN m AS seed, cluster
            """, user_id=user_id)
            
            for record in clusters:
                seed = record["seed"]
                cluster_nodes = record["cluster"]
                cluster_content = [node["content"] for node in cluster_nodes]
                
                # Summarize cluster content
                principle = summarize_memory_cluster(cluster_content)
                
                # Store condensed principle
                session.run("""
                CREATE (c:MemoryChunk:Memory {
                    id: randomUUID(),
                    content: $principle,
                    type: 'principle',
                    timestamp: datetime(),
                    embedding: $embedding
                })
                WITH c
                UNWIND $memory_ids AS mem_id
                MATCH (m:Memory {id: mem_id})
                CREATE (m)-[:SUMMARIZED_IN]->(c)
                """, principle=principle, 
                    embedding=generate_embedding(principle),
                    memory_ids=[node.id for node in [seed] + cluster_nodes])

    # === NEW FEATURE 2: CAUSAL REASONING ===
    def link_causal_memories(self, user_id, new_memory_id, content):
        """Add causal relationships for important memories"""
        with self.driver.session() as session:
            session.run("""
            MATCH (m:Memory {id: $new_memory_id})
            WITH m
            MATCH (prev:Memory)<-[:CREATED]-(:User {id: $user_id})
            WHERE prev.timestamp < m.timestamp
            
            // Find memories from past 7 days by default
            AND prev.timestamp > datetime() - duration('P7D')
            AND NOT exists((prev)-[:CAUSALLY_RELATED]->(m))
            WITH prev ORDER BY prev.timestamp DESC 
            LIMIT 3
            
            // Create relationship with confidence based on semantic similarity
            WITH m, prev, 
                 gds.similarity.cosine(m.embedding, prev.embedding) AS sim
            WHERE sim > 0.6
            MERGE (prev)-[r:CAUSALLY_RELATED]->(m)
            SET r.confidence = sim * 0.9,
                r.detected_at = datetime(),
                r.reason = 'Automatic causal linking'
            """, new_memory_id=new_memory_id, user_id=user_id)

    # === NEW FEATURE 3: CONTRADICTION DETECTION ===
    def detect_contradictions(self, user_id, limit=20):
        """Identify and flag conflicting memories"""
        with self.driver.session() as session:
            # Find memory pairs that might conflict
            potential_conflicts = session.run("""
            MATCH (m1:Memory)<-[:CREATED]-(:User {id: $user_id})
            MATCH (m2:Memory)<-[:CREATED]-(:User {id: $user_id})
            WHERE m1 <> m2
            AND NOT exists((m1)-[:CONTRADICTS]-(m2))
            AND datetime() > m1.timestamp + duration('P2D')  // Only older memories
            AND datetime() > m2.timestamp + duration('P2D')
            AND gds.similarity.cosine(m1.embedding, m2.embedding) > 0.5
            RETURN m1.id AS id1, m2.id AS id2, m1.content AS content1, m2.content AS content2
            LIMIT $limit
            """, user_id=user_id, limit=limit)
            
            for record in potential_conflicts:
                response = openai.chat.completions.create(
                    model=os.getenv("OPENROUTER_DEFAULT_MODEL", "gpt-4o-mini-2024-07-18"),
                    messages=[{
                        "role": "system",
                        "content": "Determine if these statements conflict. Answer ONLY 'yes' or 'no':"
                    }, {
                        "role": "user",
                        "content": f"Statement 1: {record['content1']}\n\nStatement 2: {record['content2']}"
                    }]
                )
                
                if "yes" in response.choices[0].message.content.lower():
                    session.run("""
                    MATCH (m1:Memory {id: $id1})
                    MATCH (m2:Memory {id: $id2})
                    MERGE (m1)-[r:CONTRADICTS]->(m2)
                    SET r.detected_at = datetime(),
                        r.confidence = 0.8,
                        r.type = 'auto-detected'
                        
                    // Adjust memory confidence
                    SET m1.confidence = m1.confidence * 0.85
                    SET m2.confidence = m2.confidence * 0.85
                    """, id1=record["id1"], id2=record["id2"])

    # === PERFORMANCE OPTIMIZATION ===
    def optimize_connections(self, user_id):
        """Clean up and strengthen connections"""
        with self.driver.session() as session:
            # Strengthen frequently accessed connections
            session.run("""
            MATCH (m1)-[r:ASSOCIATED_WITH]->(m2)
            WHERE m1.access_count > 3 AND m2.access_count > 3
            SET r.strength = min(1.0, r.strength * 1.1)
            """)
            
            # Remove weak temporal links
            session.run("""
            MATCH (m1)-[r:ASSOCIATED_WITH]->(m2)
            WHERE r.strength < 0.3 AND duration.between(datetime(), r.created_at).days > 7
            DELETE r
            """)

    def prune_low_confidence_memories(self):
        """Remove low-confidence memories weekly"""
        with self.driver.session() as session:
            session.run("""
            MATCH (m:Memory)
            WHERE m.confidence < 0.3
            AND datetime() > m.timestamp + duration('P7D')
            DETACH DELETE m
            """)
```

### utils.py Additions
```python
# Add this to utils.py
import openai

def summarize_memory_cluster(memories):
    """Condense related memories into core principles"""
    max_length = min(4000, int(len(' '.join(memories)) * 0.3))  # Limit input size
    truncated = [mem[:max_length//len(memories)] for mem in memories]
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "system",
            "content": "Extract 3 concise axiomatic principles from these memories:"
        }, {
            "role": "user",
            "content": "\n\n".join(f"- {m}" for m in truncated)
        }],
        max_tokens=350
    )
    return response.choices[0].message.content.strip()
```

### main.py Modifications
```python
# Add this at the top
import threading
from memory import Neo4jMemory

# Initialize memory system with background worker
memory = Neo4jMemory()  # Now starts background tasks automatically

# In your chat_interface function:
def chat_interface():
    # ... existing code ...
    
    # Prefetch context from principle chunks first
    if prompt:
        with st.spinner("ðŸ§  Context Optimization..."):
            context = memory.get_relevant_memories(prompt, DEFAULT_USER, strategy="principle_first")
            st.success(f"Loaded {len(context)} optimized contexts!")

# New retrieval strategy in memory.py (add to Neo4jMemory class)
def get_relevant_memories(self, query, user_id, limit=7, strategy="fast"):
    if strategy == "principle_first":
        return self.get_principle_based_context(query, user_id, limit)
    # Fallback to default method...

def get_principle_based_context(self, query, user_id, limit):
    """Retrieve principles first for faster loading"""
    query_embedding = generate_embedding(query)
    
    with self.driver.session() as session:
        # First try to get matching principles
        principles = session.run("""
        CALL db.index.vector.queryNodes('memory_embeddings', $limit, $query_embedding) 
        YIELD node, score
        WHERE node.type = 'principle'
        RETURN node.content AS content, score
        ORDER BY score DESC
        LIMIT $limit
        """, query_embedding=query_embedding, limit=limit).records()
        
        if principles:
            return [record['content'] for record in principles]
        
        # Fallback to regular memory search
        return session.run("""
        CALL db.index.vector.queryNodes('memory_embeddings', $limit, $query_embedding) 
        YIELD node, score
        RETURN node.content AS content
        ORDER BY score DESC
        """, query_embedding=query_embedding, limit=limit).records()
```

### .env Additions
```env
# Add these optional settings
OPTIMIZE_BACKGROUND=true
NIGHTLY_MAINTENANCE_WINDOW=3:00-5:00

# For GPT-powered features
OPENROUTER_DEFAULT_MODEL="gpt-4o-mini-2024-07-18"
CONTRADICTION_LIMIT=15
```

### Deployment Checklist
1. **In Replit**
   - Update files with above content
   - Add packages: `pip install pytz`
   - Restart the Repl

2. **Verify Background Tasks**
   ```python
   # Temporarily add for testing
   def test_background_tasks():
       print("Starting background test...")
       memory.optimize_connections("default_user")
       print("Optimization complete!")
       memory.create_memory_chunks("default_user")
       print("Memory compression done!")
       memory.detect_contradictions("default_user")
       print("Contradiction check finished!")
   
   # Call manually in main.py during testing
   if __name__ == "__main__":
       test_background_tasks()
   ```

3. **Performance Monitoring**
   ```python
   # Add to UI sidebar (main.py)
   def show_perf_stats():
       with st.sidebar.expander("âš¡ Performance Metrics"):
           with memory.driver.session() as session:
               stats = session.run("""
               CALL db.stats.retrieve("GRAPH COUNTS")
               YIELD data
               RETURN data.nodes AS nodes, data.relationships AS rels
               UNION
               CALL db.stats.retrieve("QUERIES")
               YIELD data
               RETURN data.avgQueryTime AS avg_query_time
               """)
               for record in stats:
                   st.metric("Total Nodes", record.get("nodes", 0))
                   st.metric("Relationships", record.get("rels", 0))
                   st.metric("Avg Query Time", f"{record.get('avg_query_time', 0):.3f}s")
   
   # Call in main interface
   if check_login():
       show_perf_stats()
       chat_interface()
   ```

### Testing Commands
After implementation, verify with:
```bash
# 1. Force temporary connection (Replit console)
python -c "from memory import Neo4jMemory; m = Neo4jMemory(); m.link_causal_memories('default', 'memory_id', 'Sample content')"

# 2. Test compression manually
python -c "from memory import Neo4jMemory; m = Neo4jMemory(); m.create_memory_chunks('default')"

# 3. Check Neo4j Browser for:
MATCH (c:MemoryChunk) RETURN c.content
MATCH ()-[r:CAUSALLY_RELATED]-() RETURN count(r)
MATCH ()-[r:CONTRADICTS]-() RETURN count(r)
```

This implementation maintains simplicity while adding significant intelligence:
1. Nightly memory compression
2. Automatic causal linking
3. Contradiction detection
4. Continuous optimization

The background worker handles all maintenance, ensuring real-time chat performance remains fast while gradually improving the system. Expect response times to decrease as principles replace raw chat context!